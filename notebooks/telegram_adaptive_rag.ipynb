{
 "cells": [
  {
   "cell_type": "code",
   "id": "8f317781-b2c4-4d16-959f-bd9f46670712",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "app_path = os.path.abspath('..')\n",
    "sys.path.insert(0, app_path)\n",
    "\n",
    "import math\n",
    "from datetime import datetime, UTC\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "from app.chroma_client import get_embeddings, get_client, Document"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2726e596170995ec",
   "metadata": {},
   "source": [
    "channel_name = 'go_to_vilnius'\n",
    "channel_id = -1133953167"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c07b9ed61b941097",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# ru_model_name = \"cointegrated/rubert-tiny2\"\n",
    "ru_model_name = \"intfloat/multilingual-e5-large\"\n",
    "embeddings = get_embeddings(ru_model_name)\n",
    "\n",
    "chroma_client_from_telegram = get_client(f'telegram_{channel_name}', embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28b19e1229e17113",
   "metadata": {},
   "source": [
    "llm_qwen3_8b = ChatOllama(model=\"qwen3:8b\")\n",
    "llm_llama_3b = ChatOllama(model=\"llama3.2:3b\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "397ac53c6414efda",
   "metadata": {},
   "source": [
    "# class UserQueryCategories(BaseModel):\n",
    "#     category: str = Field(\n",
    "#         description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\",\n",
    "#         example=\"Factual\"\n",
    "#     )\n",
    "\n",
    "class UserQueryCategories(BaseModel):\n",
    "    category: str = Field(\n",
    "        description=\"Категория запроса, возможные варианты: Фактический, Аналитический, Оценочный, Контекстуальный, Агрегированный и Обобщающий\",\n",
    "        example=\"Фактический\"\n",
    "    )\n",
    "\n",
    "class UserQueryClassifier:\n",
    "    def __init__(self, llm_model: ChatOllama = None):\n",
    "        self.llm = llm_model or ChatOllama(model=\"llama3.2:3b\")\n",
    "        # self.prompt = PromptTemplate(\n",
    "        #     input_variables=[\"query\"],\n",
    "        #     template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
    "        # )\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"query\"],\n",
    "            template=\"\"\"Вы — умный классификатор пользовательских запросов.\n",
    "Вам нужно отнести входящее сообщение ровно к одной из категорий:\n",
    "\n",
    "- Фактический\n",
    "- Аналитический\n",
    "- Оценочный\n",
    "- Контекстуальный\n",
    "- Агрегированный\n",
    "- Обобщающий\n",
    "\n",
    "Примеры:\n",
    "\n",
    "• «Когда последний день подачи налоговой декларации?» → Фактический\n",
    "• «Как получить “детские деньги”?» → Аналитический\n",
    "• «Какие самые позитивные сообщения в этом чате?» → Оценочный\n",
    "• «Какие основные проблемы у мигрантов на Кипре?» → Контекстуальный\n",
    "• «Сколько сообщений с хэштегом #вакансии опубликовано за март?» → Агрегированный\n",
    "• «Какие основные топики обсуждаются в этом чате?» → Обобщающий\n",
    "\n",
    "Теперь классифицируй следующее сообщение:\n",
    "Запрос: «{query}»\n",
    "Категория:\"\"\"\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm.with_structured_output(UserQueryCategories)\n",
    "\n",
    "    def classify(self, query):\n",
    "        category = self.chain.invoke(query).category\n",
    "        print(f\"Query {query} classified as: {category}\")\n",
    "        return category"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91a853b51950924f",
   "metadata": {},
   "source": [
    "class BaseRetrievalStrategy:\n",
    "    def get_disclamer(self):\n",
    "        return ''\n",
    "    \n",
    "    def retrieve(self, query) -> list[tuple[Document, float]]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class SimpleRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db: Chroma):\n",
    "        self.k = 10\n",
    "        self.db = db\n",
    "\n",
    "    def retrieve(self, query) -> list[tuple[Document, float]]:\n",
    "        print(\"retrieving from DB only\")\n",
    "        docs_with_score = self.db.similarity_search_with_relevance_scores(query, k=self.k)\n",
    "        return docs_with_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ce969d01d95e8fa",
   "metadata": {},
   "source": [
    "class SubQueries(BaseModel):\n",
    "    sub_queries: list[str] = Field(\n",
    "        description=\"List of sub-queries for comprehensive analysis\",\n",
    "        example=[\"What is the population of New York?\", \"What is the GDP of New York?\"]\n",
    "    )\n",
    "\n",
    "class HypotheticalQuestionsRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db: Chroma, multi_shot_retrieval = True):\n",
    "        self.k = 10\n",
    "        self.db = db\n",
    "        self.llm = llm_llama_3b\n",
    "        self.multi_shot_retrieval = multi_shot_retrieval\n",
    "\n",
    "    def similarity_search_for_each_question(self, queries) -> list[tuple[Document, float]]:\n",
    "        docs_with_score = []\n",
    "        k = math.ceil(self.k / len(queries))\n",
    "        for query in queries:\n",
    "            docs_with_score.extend(self.db.similarity_search_with_relevance_scores(query, k=k))\n",
    "            print(f'Query {query} related docs: {'\\n'.join(str(docs_with_score))}')\n",
    "\n",
    "        return docs_with_score\n",
    "\n",
    "    def similarity_search_all_at_once(self, queries) -> list[tuple[Document, float]]:\n",
    "        docs_with_score = []\n",
    "        query = ' '.join(queries)\n",
    "        docs_with_score = self.db.similarity_search_with_relevance_scores(query, k=self.k)\n",
    "        print(f'Query {query} related docs: {docs_with_score}')\n",
    "\n",
    "        return docs_with_score\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        print(\"retrieving analytical\")\n",
    "        # sub_queries_prompt = PromptTemplate(\n",
    "        #     input_variables=[\"query\", \"k\"],\n",
    "        #     template=\"Generate {k} sub-questions for: {query}\"\n",
    "        # )\n",
    "        sub_queries_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"k\"],\n",
    "            template=\"Проанализируй следующий вопрос пользователя. \\\n",
    "Сгенерируй {k} альтернативных формулировок или связанных под-вопросов (sub_queries), которые рассматривают этот вопрос с разных сторон \\\n",
    "(например, уточняют детали, рассматривают причины, следствия, альтернативные сценарии и т.д.). \\\n",
    "Эти вопросы будут использованы для улучшения поиска в векторной базе знаний. \\\n",
    "Вопрос пользователя: '{query}'\\n\\n\\\n",
    "Альтернативные вопросы (sub_queries):\",\n",
    "        )\n",
    "\n",
    "        sub_queries_chain = sub_queries_prompt | self.llm.with_structured_output(SubQueries)\n",
    "\n",
    "        input_data = {\"query\": query, \"k\": self.k}\n",
    "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
    "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
    "\n",
    "        queries = sub_queries\n",
    "        queries.append(query)\n",
    "        print(queries)\n",
    "        if self.multi_shot_retrieval:\n",
    "            docs_with_score = self.similarity_search_for_each_question(queries)\n",
    "        else:\n",
    "            docs_with_score = self.similarity_search_all_at_once(queries)\n",
    "\n",
    "        return docs_with_score\n",
    "\n",
    "    # def apply_diversity():\n",
    "    #     # Use LLM to ensure diversity and relevance\n",
    "    #     diversity_prompt = PromptTemplate(\n",
    "    #         input_variables=[\"query\", \"docs\", \"k\"],\n",
    "    #         template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
    "    #         Return only the indices of selected documents as a list of integers.\"\"\"\n",
    "    #     )\n",
    "    #     diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
    "    #     docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
    "    #     input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
    "    #     selected_indices_result = diversity_chain.invoke(input_data).indices\n",
    "    #     print(f'selected diverse and relevant documents')\n",
    "\n",
    "    #     return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84747b21",
   "metadata": {},
   "source": [
    "class HypotheticalAnswers(BaseModel):\n",
    "    answers: list[str] = Field(\n",
    "        description=\"List of hypothetical answers for better information retrieval from vector database\",\n",
    "        # example=[\"What is the population of New York?\", \"What is the GDP of New York?\"]\n",
    "    )\n",
    "\n",
    "class HypotheticalAnswersRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self, db: Chroma):\n",
    "        self.k = 8\n",
    "        self.db = db\n",
    "        self.llm = llm_qwen3_8b  # llm_llama_3b\n",
    "\n",
    "    def similarity_search_all_at_once(self, queries) -> list[tuple[Document, float]]:\n",
    "        docs_with_score = []\n",
    "        # if isinstance(queries, list):\n",
    "            # query = '/n'.join(queries)\n",
    "        query = queries\n",
    "        docs_with_score = self.db.similarity_search_with_relevance_scores(query, k=self.k)\n",
    "        print(f'Query {query} related docs: {docs_with_score}')\n",
    "\n",
    "        return docs_with_score\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        prompt_expanding = PromptTemplate(\n",
    "            input_variables=[\"user_query\", \"k\"],\n",
    "            template=\"\"\"\n",
    "Контекст данных:\n",
    "– Пользовательский запрос: {user_query}\n",
    "\n",
    "Задача:\n",
    "Сгенерировать ровно {k} гипотетических, но практически релевантных ответов (answers) на основе вышеуказанного вопроса.\n",
    "\n",
    "Требования к каждому ответу:\n",
    "1. Длина не более 2–3 предложений.\n",
    "2. Формат выходных данных: маркированный.\n",
    "3. Не добавлять заголовки, авторов, ссылки или другие метаданные.\n",
    "\n",
    "Пример:\n",
    "Вопрос: «Какие самые позитивные сообщения в чате?»\n",
    "Ответ:\n",
    "- Ого, какая замечательная идея! Это точно поднимет настроение всем!\n",
    "- Спасибо за вашу помощь — вы просто спасли мой день!\n",
    "- Мне очень нравится, как вы подходите к решению задач: вдохновляет!\n",
    "- Ваше сообщение заставило меня улыбнуться — вы лучик света в этом чате!\n",
    "- Благодарю за поддержку, с вами так легко общаться!\n",
    "\n",
    "Гипотетические ответы (answers):\"\"\",\n",
    "        )\n",
    "\n",
    "        chain = prompt_expanding | self.llm.with_structured_output(HypotheticalAnswers)\n",
    "\n",
    "        input_data = {\"user_query\": query, \"k\": self.k}\n",
    "        additional_answers = chain.invoke(input_data).answers\n",
    "        print(f'generated hypothetical answers: {additional_answers}')\n",
    "\n",
    "        user_query_expanded = f'{query}\\n\\n{additional_answers}'\n",
    "        print(user_query_expanded)\n",
    "        docs_with_score = self.similarity_search_all_at_once(user_query_expanded)\n",
    "\n",
    "        return docs_with_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1837359a",
   "metadata": {},
   "source": [
    "class GraphRetrievalStrategy(SimpleRetrievalStrategy):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_disclamer(self):\n",
    "        return '**The system has not implemented Query-Focused Summarization and Generalization yet.\\n\\\n",
    "The follow answer is base only on simple retrival from DB and can be low quality.**\\n'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d238ce7",
   "metadata": {},
   "source": [
    "class AggregationRetrievalStrategy(BaseRetrievalStrategy):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_disclamer(self):\n",
    "        return '**The system has not implemented Aggregation yet.**\\n'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e297c8c001160326",
   "metadata": {},
   "source": [
    "user_query = \"Какие самые токсичные сообщения в чате?\"\n",
    "# user_query = \"Какие частые горячие темы в этом чате?\"\n",
    "# user_query = \"Что известно о непродлении ВНЖ?\"\n",
    "# user_query = \"Собери и обобщи всю информацию касаемо саун?\"\n",
    "# user_query = \"Какие самые негативные сообщения с Литве и Вильнюсе в чате.\"\n",
    "# user_query = \"Можно ли компенсировать отель или аквапарк через велнес/БТА/бта/BTA?\"\n",
    "# user_query = \"Как получить детские деньги или компенсацию по уходу за ребенком.\"\n",
    "# user_query = \"Какой процент сообщений в этом чате?\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74ad181f10ac031c",
   "metadata": {},
   "source": [
    "classifier = UserQueryClassifier(llm_llama_3b)\n",
    "query_type = classifier.classify(user_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e89b342-185e-4dc2-aa83-3413554863a1",
   "metadata": {},
   "source": [
    "strateg = HypotheticalAnswersRetrievalStrategy(db=chroma_client_from_telegram)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5bb472a-4745-4bb4-b7dc-f2ee940efa15",
   "metadata": {},
   "source": [
    "strateg.retrieve(user_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "004a6ab9-dd73-4576-b3b6-2c2ca067db41",
   "metadata": {},
   "source": [
    "class AdaptiveRAG:\n",
    "    def __init__(self, classifier, db):\n",
    "        self.classifier = classifier\n",
    "        self.strategies_to_query_type = {\n",
    "            'Фактический': SimpleRetrievalStrategy(db=db),\n",
    "            'Аналитический': HypotheticalQuestionsRetrievalStrategy(db=db, multi_shot_retrieval=False),\n",
    "            'Оценочный': HypotheticalAnswersRetrievalStrategy(db=db),\n",
    "            'Контекстуальный': HypotheticalQuestionsRetrievalStrategy(db=db),\n",
    "            'Обобщающий': GraphRetrievalStrategy(),\n",
    "            'Агрегированный': AggregationRetrievalStrategy(),\n",
    "        }\n",
    "        self.answer_template = \"Query was classified as {query_type}.\\n {disclamer}{answer}\"\n",
    "        self.llm = llm_qwen3_8b\n",
    "        \n",
    "    def get_answer(self, user_query: str) -> str:\n",
    "        answer = \"\"\n",
    "        query_type = self.classifier.classify(user_query)\n",
    "\n",
    "        strategy = self.strategies_to_query_type.get(query_type)\n",
    "        if not strategy:\n",
    "            answer = \"We can't classify your query. Try to refolmulate your question.\"\n",
    "            return answer\n",
    "\n",
    "        disclamer = strategy.get_disclamer()\n",
    "        docs_with_score = strategy.retrieve(user_query)\n",
    "        if not docs_with_score:\n",
    "            return self.answer_template.format(query_type=query_type, disclamer=disclamer, answer='')\n",
    "        \n",
    "        filtered_related_docs = filter(lambda doc_score: doc_score[-1] > 0.3, docs_with_score)\n",
    "        context = \"\\n\\n---\\n\\n\".join(\n",
    "            f\"{datetime.fromtimestamp(doc.metadata['date'], UTC)} - {doc.page_content}\" for doc, _score in filtered_related_docs\n",
    "        )\n",
    "\n",
    "        final_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"user_query\"],\n",
    "        template=\"\"\"\n",
    "Ты полезный AI ассистент, который отвечает на вопросы пользователя на основе контекста.\n",
    "Контекст это релевантные вопросу сообщения из телеграм чата.\n",
    "\n",
    "Контекст:\n",
    "{context}\n",
    "\n",
    "Вопрос пользователя:\n",
    "{user_query}\n",
    "\n",
    "Если в контексте недостаточно информации, чтобы ответить на вопрос пользователя, то скажи, что недостаточно информации.\n",
    "\n",
    "Ответ:\"\"\",\n",
    "        )\n",
    "\n",
    "        chain = final_prompt | self.llm\n",
    "        llm_response = chain.invoke({\"user_query\": user_query, \"context\": context}).content\n",
    "        # return query_type, disclamer, llm_response\n",
    "        llm_response = llm_response.split('</think>', 1)[-1]\n",
    "        return self.answer_template.format(query_type=query_type, disclamer=disclamer, answer=llm_response)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83d7b342-8e54-449d-be70-7b8b025c713f",
   "metadata": {},
   "source": [
    "user_query = \"Какие самые токсичные сообщения в чате?\"\n",
    "# user_query = \"Какие частые горячие темы в этом чате?\"\n",
    "user_query = \"Что известно о не продлении ВНЖ?\"\n",
    "# user_query = \"Собери и обобщи всю информацию касаемо саун?\"\n",
    "# user_query = \"Какие самые негативные сообщения с Литве и Вильнюсе в чате.\"\n",
    "user_query = \"Можно ли компенсировать отель или аквапарк через велнес/БТА/бта/BTA?\"\n",
    "user_query = \"Как получить детские деньги или компенсацию по уходу за ребенком.\"\n",
    "user_query = \"Расскажи, что ты знаешь про получение детских денег или компенсации по уходу за ребенком.\"\n",
    "user_query = \"Сколько экстремистов в этом чате?\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2d6790f-d5dd-440b-8e1d-67b3304791cc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "rag = AdaptiveRAG(classifier=classifier, db=chroma_client_from_telegram)\n",
    "response = rag.get_answer(user_query)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "896c7b90-a686-4aed-a27d-2e5a8807f165",
   "metadata": {},
   "source": [
    "response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92d634dd-28ea-49b9-8880-7e1808a8d897",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f79e6467-bf85-48fe-9e3a-fd3aa62f07ea",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
