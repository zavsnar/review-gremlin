{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4171003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "app_path = os.path.abspath('..')\n",
    "sys.path.insert(0, app_path)\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from langchain_core.documents import Document # Keep Document as it's used in the query function signature if needed later\n",
    "from langchain import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from app.chroma_client import chroma_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a494bce-f8a1-479c-886c-f8cf3250becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI = \"mongodb://127.0.0.1:27017/mydb\"  # Use the service name 'mongo'\n",
    "mongo_client = MongoClient(MONGODB_URI)\n",
    "mdb = mongo_client.mydb\n",
    "mongo_collection = mdb.mycollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec9677ea-d3e1-44e7-ab39-8644a185ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the most toxic comments?\"\n",
    "separator = \"#_next_#\"\n",
    "expand_to_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1b267b-0d6a-4c88-adfc-03533185f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama_3b = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9258fffb-d754-4d29-8887-b92eda21e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_r1_8b = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30d55db-0232-498f-845c-3b6d61a4def6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Comments with phrases like \"this is stupid\" or \"you\\'re wrong\"\\n2. Comments containing insults such as \"idiot\" or \"unprofessional\"\\n3. Posts stating that a contributor\\'s work is not good enough\\n4. Repeated comments asking the same question over and over again\\n5. Comments claiming that someone is \"too slow\" or \"can\\'t keep up\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"expand_to_n\"],\n",
    "    template=\"\"\"\n",
    "    You have information about comments on pull requests.\n",
    "    Given the question '{user_query}', generate {expand_to_n} hypothetical answers that directly answer this question.\n",
    "    \n",
    "    Output ONLY the hypothetical answers without any other text, explanations, authors, or headers.\n",
    "    \"\"\",\n",
    ")\n",
    "hyde_chain = hyde_prompt | llm_llama_3b\n",
    "raw_additional_questions = hyde_chain.invoke({\"user_query\": user_query, \"expand_to_n\": expand_to_n}).content\n",
    "raw_additional_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa5f2184-34c0-4618-940f-d2e8fbd5dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"{user_query}/n/n{raw_additional_questions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380eee43-5113-44f1-8eec-19c8e85b2693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.11732434145706727 - This is a solid contribution. Thanks for your hard work!\n",
      " --- \n",
      "Score: -0.13601440256062358 - Please reformat this section to adhere to PEP 8 guidelines.\n",
      " --- \n",
      "Score: -0.14860558640591148 - The comments in this file are outdated.\n",
      " --- \n",
      "Score: -0.15707402720083574 - The logging in this section is not very informative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/24vfwvtn7638ygdrlqbgcpl80000gn/T/ipykernel_6027/1849491603.py:1: UserWarning: Relevance scores must be between 0 and 1, got [(Document(id='aa1055d8-79c2-49bb-887d-9642d44b32ef', metadata={}, page_content='This is a solid contribution. Thanks for your hard work!'), -0.11732434145706727), (Document(id='4c7353c3-1c84-49d0-8ad8-438e1b3f85a6', metadata={}, page_content='Please reformat this section to adhere to PEP 8 guidelines.'), -0.13601440256062358), (Document(id='670b9bf1-f6cb-4764-90fa-e7e3cbf7d54b', metadata={}, page_content='The comments in this file are outdated.'), -0.14860558640591148), (Document(id='8d5c02a1-d899-437c-ad0c-92d5339294b2', metadata={}, page_content='The logging in this section is not very informative.'), -0.15707402720083574)]\n",
      "  results = chroma_client.similarity_search_with_relevance_scores(query)\n"
     ]
    }
   ],
   "source": [
    "results = chroma_client.similarity_search_with_relevance_scores(query)\n",
    "\n",
    "context_comments = \"\\n --- \\n\".join([f\"Score: {score} - {doc.page_content}\" for doc, score in results])\n",
    "print(context_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6158b22-a201-4891-95c2-86afe38bedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_query = f\"{user_query}/n/n{raw_additional_questions}\"\n",
    "print(augmented_query)\n",
    "\n",
    "GENERATE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant analyzing Fisheye/Stash code review comments.\n",
    "Based solely on the following comments provided as context, please answer the user's question.\n",
    "If the comments don't provide enough information, state that.\n",
    "\n",
    "Context Comments:\n",
    "{context_comments}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d26f4b37-fd9a-414f-9f7f-8cce2d56fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPANTION_PROMPT_TEMPLATE = f\"\"\"You are an AI language model assistant.\n",
    "Your task is to generate {expand_to_n} different versions of the given user question or sentence to \n",
    "retrieve relevant documents from a vector database.\n",
    "By generating multiple perspectives on the user question or sentence, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Database stores comments to pull-requests.\n",
    "Provide these alternative questions or sentences seperated by '{separator}'. \n",
    "Don't add to your answer additional information.\n",
    "Original question: {user_query}\n",
    "\n",
    "Additional questions:\n",
    "\"\"\"\n",
    "\n",
    "EXPANTION_WITH_EXAMPLES_PROMPT_TEMPLATE = f\"\"\"\n",
    "You have information about comments on pull requests.\n",
    "Generate {expand_to_n} concrete example answers for the following question.\n",
    "\n",
    "Original question: {user_query}\n",
    "\n",
    "Output ONLY the example answers. \n",
    "Do not include any other text, explanations, or headers.\n",
    "\n",
    "For example:\n",
    "    QUESTION:\n",
    "    What are the most negative comments?\n",
    "    RESPONSE:\n",
    "    1. This code will never compile due to the mismatched syntax\n",
    "    2. This bug will cause issues in production\n",
    "    3. Lack of unit tests\n",
    "\"\"\"\n",
    "\n",
    "EXPANTION_WITH_EXAMPLES_PROMPT_TEMPLATE = f\"\"\"\n",
    "You have information about comments in Instagram.\n",
    "Generate {expand_to_n} concrete example answers for the following question.\n",
    "\n",
    "Original question: {user_query}\n",
    "\n",
    "Output ONLY the example answers. \n",
    "Do not include any other text, explanations, or headers.\n",
    "\"\"\"\n",
    "\n",
    "EXPANTION_WITH_EXAMPLES_PROMPT_TEMPLATE = f\"\"\"\n",
    "You have information about comments on pull requests.\n",
    "Given the question '{user_query}', generate {expand_to_n} hypothetical answers that directly answer this question.\n",
    "\n",
    "Output ONLY the hypothetical answers. \n",
    "Do not include any other text, explanations, or headers.\n",
    "\"\"\"\n",
    "\n",
    "prompt = EXPANTION_WITH_EXAMPLES_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c56a2a56-d985-44c1-ad4e-dab6f68059e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_prompt = PromptTemplate(\n",
    "    input_variables=[\"user_query\", \"expand_to_n\"],\n",
    "    template=\"\"\"\n",
    "    You have information about comments on pull requests.\n",
    "    Given the question '{user_query}', generate {expand_to_n} hypothetical answers that directly answer this question.\n",
    "    \n",
    "    Output ONLY the hypothetical answers without any other text, explanations, authors, or headers.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "809c3b13-6746-4d5e-9a74-0094b3edb5e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_llama_3b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hyde_chain = hyde_prompt | \u001b[43mllm_llama_3b\u001b[49m\n\u001b[32m      2\u001b[39m raw_additional_questions = hyde_chain.invoke({\u001b[33m\"\u001b[39m\u001b[33muser_query\u001b[39m\u001b[33m\"\u001b[39m: user_query, \u001b[33m\"\u001b[39m\u001b[33mexpand_to_n\u001b[39m\u001b[33m\"\u001b[39m: expand_to_n}).content\n\u001b[32m      3\u001b[39m raw_additional_questions\n",
      "\u001b[31mNameError\u001b[39m: name 'llm_llama_3b' is not defined"
     ]
    }
   ],
   "source": [
    "hyde_chain = hyde_prompt | llm_llama_3b\n",
    "raw_additional_questions = hyde_chain.invoke({\"user_query\": user_query, \"expand_to_n\": expand_to_n}).content\n",
    "raw_additional_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6271ce42-a2f5-45dd-a4fc-831b67ba1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_r1_8b = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2169824-87aa-42dd-b373-93ca76b18f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, so I need to figure out how to approach this user\\'s query. They provided a specific instruction where they want me to generate five hypothetical answers to the question \\'What is the most positive comments?\\' based on information about comments on pull requests.\\n\\nFirst, I should understand what exactly they\\'re looking for. They mentioned that the response should only contain the hypothetical answers without any other text or explanations. So my task is straightforward: create five concise and varied responses that directly address what makes a comment the most positive in this context.\\n\\nI need to consider what aspects make a comment positive. Factors could include being encouraging, constructive, specific, genuine, or supportive. Each response should highlight different qualities to ensure diversity.\\n\\nI\\'ll start by brainstorming words or phrases related to positivity: uplifting, encouraging, constructive, genuine, supportive, meaningful, thoughtful, specific, kind, and so on. Then, I can combine these into complete sentences that clearly express the positive trait being discussed.\\n\\nI should make sure each answer is unique and offers a different perspective on what makes a comment positive. This will give the user a comprehensive set of responses to choose from or use as needed.\\n\\nLet me draft them one by one, ensuring they are clear and direct. I\\'ll also check that each response is concise without unnecessary words.\\n</think>\\n\\n1. \"The most positive comments are those that are uplifting and genuinely encouraging.\"\\n2. \"Positive comments are those that provide constructive feedback and offer specific praise.\"\\n3. \"The most positive comments are those that are thoughtful, kind, and show genuine support.\"\\n4. \"A positive comment is one that is supportive, meaningful, and leaves the recipient feeling motivated.\"\\n5. \"Most positive comments are those that are detailed, specific, and clearly expressed.\"'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_chain_r1_8b = hyde_prompt | llm_r1_8b\n",
    "raw_additional_questions = hyde_chain_r1_8b.invoke({\"user_query\": user_query, \"expand_to_n\": expand_to_n}).content\n",
    "raw_additional_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c426e58-ebbc-473f-9592-0bd0522524d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _llm_response = llm_model.invoke(prompt)\n",
    "# raw_additional_questions = _llm_response.content\n",
    "# additional_questions = _llm_response.content.split(separator)[1:]\n",
    "# raw_additional_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe787772-90cc-47f3-ac46-07fd499c8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question in additional_questions:\n",
    "#     print(question)\n",
    "\n",
    "# additional_questions_str = \" \".join(additional_questions).replace('\\n\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae91711-d191-4b64-909d-0a952f7a84ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the most positive comments? 1. \"Great job on implementing the new feature! The code looks clean and well-organized.\"\n",
      "2. \"I love how you handled the edge case. It's a great example of robust error handling.\"\n",
      "3. \"The documentation is excellent. You've done a great job making it easy for others to understand the code.\"\n",
      "4. \"This pull request has greatly improved our team's workflow. Well done!\"\n",
      "5. \"Your explanation of the code changes was clear and concise, thanks for taking the time to write it.\"\n"
     ]
    }
   ],
   "source": [
    "augmented_query = f\"{user_query} {raw_additional_questions}\"\n",
    "print(augmented_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41395f-1d9b-46d9-90b2-50ebc29919d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma_client.similarity_search(augmented_query)\n",
    "\n",
    "context_comments = \"\\n\\n --- \\n\\n\".join([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687ec80-26a3-4e7d-853b-94de3dc4d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job on this pull request! The code is clean and well-documented.\n",
      "\n",
      " --- \n",
      "\n",
      "The user interface changes look good.\n",
      "\n",
      " --- \n",
      "\n",
      "I'm impressed with the quality of this code. Keep up the good work!\n",
      "\n",
      " --- \n",
      "\n",
      "The comments in this file are outdated.\n"
     ]
    }
   ],
   "source": [
    "print(context_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef93895-944e-4e83-9fdb-f55e7837dd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
