{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f5a511-7671-4141-8d9e-03cb9d647bf5",
   "metadata": {},
   "source": [
    "# Hypothetical Prompt Embeddings (HyPE)"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f317781-b2c4-4d16-959f-bd9f46670712",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "app_path = os.path.abspath('..')\n",
    "sys.path.insert(0, app_path)\n",
    "\n",
    "from datetime import datetime, UTC, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain import PromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from app.chroma_client import get_embeddings, get_client"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2726e596170995ec",
   "metadata": {},
   "source": [
    "channel_name = 'relocationexpert'\n",
    "channel_id = 'relocationexpert'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c07b9ed61b941097",
   "metadata": {},
   "source": [
    "# ru_model_name = \"cointegrated/rubert-tiny2\"\n",
    "ru_model_name = \"intfloat/multilingual-e5-large\"\n",
    "embeddings_model = get_embeddings(ru_model_name)\n",
    "\n",
    "chroma_client_from_telegram = get_client(f'telegram_{channel_name}', embeddings_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28b19e1229e17113",
   "metadata": {},
   "source": [
    "llm_qwen3_8b = ChatOllama(model=\"qwen3:8b\")\n",
    "llm_llama3_3b = ChatOllama(model=\"llama3.2:3b\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3fc516d4-3b35-4a82-a916-fad50b358a05",
   "metadata": {},
   "source": [
    "## Get channel messages from vector DB.\n",
    "How messages have been added in DB see in [./telegram_crawler.ipynb](telegram_crawler.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "id": "840d2ffe",
   "metadata": {},
   "source": [
    "all_docs = chroma_client_from_telegram.get(\n",
    "    where={\"date\": {\"$gte\": (datetime.now(UTC) - timedelta(days=90)).timestamp()}},\n",
    ")\n",
    "docs_with_meta = list(zip(all_docs['metadatas'], all_docs['documents']))\n",
    "len(docs_with_meta)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d57be40c-2220-4adb-98a1-12635d1a7491",
   "metadata": {},
   "source": [
    "chroma_client_prompt_embedding = get_client(f'telegram_{channel_name}_PE', embeddings_model)\n",
    "# chroma_client_prompt_embedding._client.delete_collection(name=f'telegram_{channel_name}_PE')\n",
    "collection = chroma_client_prompt_embedding._client.get_or_create_collection(name=f'telegram_{channel_name}_PE')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd6d37bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def generate_and_add_records_to_db(meta, text):\n",
    "    examples = []\n",
    "\n",
    "    ru_telegram_prompt_expanding = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Проанализируйте текст и сгенерируйте вопросы, которые, если на них ответить, \\\n",
    "    отразят основные аспекты текста. В выводе должны быть только вопросы и ничего больше. Каждый вопрос должен быть одной строкой, без нумерации или префиксов.\\n\\n \\\n",
    "    Текст:\\n{text}\\n\\nВопросы:\\n\",\n",
    "    )\n",
    "\n",
    "    hype_chain = ru_telegram_prompt_expanding | llm_llama3_3b\n",
    "    hypothetical_questions = list(filter(\n",
    "        bool,\n",
    "        map(\n",
    "            str.strip,\n",
    "            hype_chain.invoke({\"text\": text}).content.replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
    "        ),\n",
    "   ))\n",
    "\n",
    "    print(hypothetical_questions)\n",
    "\n",
    "    question_embeddings = embeddings_model.embed_documents(hypothetical_questions)\n",
    "\n",
    "    collection.add(\n",
    "        documents=[text] * len(question_embeddings),\n",
    "        embeddings=question_embeddings,\n",
    "        metadatas=[meta] * len(question_embeddings),\n",
    "        ids=[f\"{meta['id']}_{i}\" for i in range(len(question_embeddings))],\n",
    "    )\n",
    "\n",
    "\n",
    "for meta, text in tqdm(docs_with_meta):\n",
    "    generate_and_add_records_to_db(meta, text)\n",
    "# print('/n'.join(examples))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a077f64-9614-486f-aa25-859c2d74af52",
   "metadata": {},
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as pool:\n",
    "    futures = [pool.submit(generate_and_add_records_to_db, meta, text) for meta, text in docs_with_meta]\n",
    "\n",
    "    for f in tqdm(as_completed(futures), total=len(chunks)):\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99a46356dd5b1bfb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "related_docs = chroma_client_prompt_embedding.similarity_search_with_relevance_scores(user_query, k=10)\n",
    "\n",
    "unique_docs = {}\n",
    "for doc, relevance in related_docs:\n",
    "    if doc.metadata['id'] in unique_docs:\n",
    "        continue\n",
    "    unique_docs[doc.metadata['id']] = (doc, relevance)\n",
    "\n",
    "list(unique_docs.values())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c35007263ed6b21d",
   "metadata": {},
   "source": [
    "filtered_related_docs = filter(lambda doc_score: doc_score[-1] > 0.3, unique_docs.values())\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join(f\"{datetime.fromtimestamp(doc.metadata['date'], UTC)} - {doc.page_content}\" for doc, _score in filtered_related_docs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "313cb15198b2d9a",
   "metadata": {},
   "source": [
    "user_query = \"Когда Кипр входит в Шенгенскую зону?\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bff1b4fc98c216be",
   "metadata": {},
   "source": [
    "ru_telegram_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"user_query\"],\n",
    "    template=\"\"\"\n",
    "    Ты полезный AI ассистент, который отвечает на вопросы пользователя на основе контекста.\n",
    "    Контекст это релевантные вопросу сообщения из телеграм чата.\n",
    "\n",
    "    Контекст:\n",
    "    {context}\n",
    "\n",
    "    Вопрос пользователя:\n",
    "    {user_query}\n",
    "\n",
    "    Если в контексте недостаточно информации, чтобы ответить на вопрос пользователя, то скажи, что недостаточно информации.\n",
    "\n",
    "    Ответ:\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "hyde_chain_r1_8b = ru_telegram_prompt | llm_qwen3_8b\n",
    "llm_response = hyde_chain_r1_8b.invoke({\"user_query\": user_query, \"context\": context}).content\n",
    "print(llm_response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91144e5909477f78",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
